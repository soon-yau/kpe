{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac576a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "import cv2\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "from core.segment import Segmentor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c247b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_DIM = 256 # target resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfe5c1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "language = loadmat('../data/language_original.mat')\n",
    "subset = loadmat('../data/subset_index.mat')\n",
    "\n",
    "captions = [x[0][0] for x in language['engJ']]\n",
    "image_files = [x[0][0].split('img/')[-1] for x in subset['nameList']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f41bd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dir = '../data/img_highres/'\n",
    "dst_dir = '../data/img_256/'\n",
    "mask_dir = '../data/mask_256/'\n",
    "multi_dst_dir = '../data/img_multi_256/'\n",
    "os.makedirs(multi_dst_dir)\n",
    "os.makedirs(mask_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4106ab",
   "metadata": {},
   "source": [
    "# Resize high resolution to 256x256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "51dd0ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13260it [31:50,  6.94it/s]\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=['image', 'text', 'num_people', 'keypoints', 'pose_score'])\n",
    "\n",
    "for image_fname, caption in tqdm(zip(image_files, captions)):\n",
    "    src_file = os.path.join(src_dir,image_fname)\n",
    "    dst_file = os.path.join(dst_dir,image_fname)\n",
    "    img = cv2.imread(src_file)\n",
    "    if img is None:\n",
    "        continue\n",
    "    os.makedirs(os.path.dirname(dst_file), exist_ok=True)\n",
    "    \n",
    "    h, w, _ = img.shape\n",
    "    if h>w:\n",
    "        pad = (h-w)//2\n",
    "        img = cv2.copyMakeBorder(img, top=0, bottom=0, left=pad, right=pad,\n",
    "                                       borderType=cv2.BORDER_REPLICATE)\n",
    "    elif h<w:\n",
    "        pad = w-h\n",
    "        img = cv2.copyMakeBorder(img, top=pad, bottom=0, left=0, right=0,\n",
    "                                       borderType=cv2.BORDER_REPLICATE)\n",
    "    h, w, _ = img.shape\n",
    "    \n",
    "    if h!=TARGET_DIM or w!=TARGET_DIM:\n",
    "        img = cv2.resize(img, (TARGET_DIM, TARGET_DIM), cv2.INTER_AREA)\n",
    "    \n",
    "    cv2.imwrite(dst_file, img)    \n",
    "\n",
    "    new_row = {'image':dst_file, 'text':[caption], 'num_people':1}\n",
    "    df = df.append(new_row, ignore_index=True)\n",
    "    \n",
    "df.to_pickle(\"deepfashion.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f0561da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"deepfashion.pickle\")\n",
    "\n",
    "image_files = list(df.image)\n",
    "texts = list(df.text)\n",
    "N = len(images_files)\n",
    "assert N==len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "0284ba82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "reg = re.compile('women|woman|lady|girl')\n",
    "df['female'] = df.text.map(lambda x: 1 if reg.search(x[0]) else 0)\n",
    "df['male'] = 1^ df['female']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "0238c756",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_indices = list(df[df.male==1].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b0707e",
   "metadata": {},
   "source": [
    "## Get Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac984824",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/soon/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "segmentor = Segmentor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "437ded9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 78955/78955 [53:26<00:00, 24.62it/s]\n"
     ]
    }
   ],
   "source": [
    "for image_fname in tqdm(image_files):\n",
    "    bmp_name = image_fname.replace('img_256','mask_256').replace('.jpg','.bmp')\n",
    "    os.makedirs(os.path.dirname(bmp_name), exist_ok=True)\n",
    "    img = cv2.imread(image_fname)\n",
    "    _, mask = segmentor(img)\n",
    "    cv2.imwrite(bmp_name, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "9259b0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample(sampled_idx, margin=5, mask_background=True):\n",
    "    row = df.iloc[sampled_idx]\n",
    "    image_fname = row.image #image_files[sampled_idx]\n",
    "    image = cv2.imread(image_fname)\n",
    "    bmp_name = image_fname.replace('img_256','mask_256').replace('.jpg','.bmp')\n",
    "    mask = cv2.imread(bmp_name, 0)    \n",
    "    if mask_background:\n",
    "        image = cv2.bitwise_and(image, image, mask=mask)        \n",
    "        image[mask==0] = (255, 255, 255)\n",
    "        \n",
    "    # crop \n",
    "    vertical = np.mean(mask, axis=0)\n",
    "    height, width = mask.shape\n",
    "    for w in range(width):\n",
    "        if vertical[w] > 0.1:\n",
    "            left = w\n",
    "            break\n",
    "            \n",
    "    for w in range(width-1, -1, -1):\n",
    "        if vertical[w] > 0.1:\n",
    "            right = w\n",
    "            break\n",
    "            \n",
    "    left = max(0, left-margin)\n",
    "    right = min(width, right+margin)\n",
    "    \n",
    "    return image[:,left:right], row.text[0], row.female, row.male\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "f226f89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 50000/50000 [47:36<00:00, 17.50it/s]\n",
      "100%|███████████████████████████████████| 50000/50000 [2:06:25<00:00,  6.59it/s]\n"
     ]
    }
   ],
   "source": [
    "HEIGHT = 256\n",
    "WIDTH = 256\n",
    "multi_df = pd.DataFrame(columns=['image', 'text', 'num_people', 'keypoints', 'pose_score'])\n",
    "\n",
    "empty_percent = 0.2\n",
    "male_percent = 0.2\n",
    "random.seed(888)\n",
    "np.random.seed(888)\n",
    "\n",
    "num_samples = {2: 50000, 3:50000} # samples per numbeer of people\n",
    "\n",
    "for p, num_sample in num_samples.items():\n",
    "    for idx in tqdm(range(num_sample)):\n",
    "        num_slots = p\n",
    "        empty = random.uniform(0,1) < empty_percent\n",
    "        if empty: # have empty slot\n",
    "            num_slots += 1\n",
    "\n",
    "        slots_avail = [1 for _ in range(num_slots)]\n",
    "        if empty:\n",
    "            slots_avail[random.randrange(0,num_slots,1)] = 0\n",
    "        '''\n",
    "        print(\"num_slot\", num_slots)\n",
    "        print(\"empty_slot_id\", empty_slot_id)\n",
    "        print(\"slots_avail\", slots_avail)\n",
    "        '''\n",
    "        merged = np.zeros((TARGET_DIM, TARGET_DIM, 3), dtype=np.uint8)\n",
    "        slot_width = TARGET_DIM//num_slots\n",
    "\n",
    "        slot_images = []\n",
    "        captions = \"\"\n",
    "        male_count = 0\n",
    "        female_count = 0\n",
    "        for not_empty in slots_avail:\n",
    "            if not not_empty:\n",
    "                empty_image = 255*np.ones((HEIGHT, int(WIDTH*0.4),3), dtype=np.uint8)\n",
    "                slot_images.append(empty_image)\n",
    "                continue\n",
    "            # for certain percentage, sample from male only\n",
    "            if random.uniform(0,1) < male_percent:\n",
    "                sampled_idx = random.sample(male_indices[:100], 1)[0]\n",
    "            else:\n",
    "                sampled_idx = random.randrange(N)\n",
    "            cropped_image, caption, female, male = get_sample(sampled_idx)\n",
    "            captions += caption\n",
    "            male_count += male\n",
    "            female_count += female\n",
    "            # random resize\n",
    "            scale_factor = np.random.uniform(0.9, 1.1)\n",
    "            cropped_image = cv2.resize(cropped_image, None, fx=scale_factor, fy=scale_factor,\n",
    "                                      interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "            slot_images.append(cropped_image)\n",
    "\n",
    "\n",
    "        merged = 255*np.ones((HEIGHT, WIDTH, 3), dtype=np.uint8)\n",
    "        total_width = sum(image.shape[1] for image in slot_images)\n",
    "\n",
    "        start_x = 0\n",
    "        for i, slot_image in enumerate(slot_images):\n",
    "            h, w, _ = slot_image.shape\n",
    "            scale_factor = WIDTH/total_width\n",
    "            slot_image = cv2.resize(slot_image, None, fx=scale_factor, fy=scale_factor)\n",
    "            h, w, _ = slot_image.shape\n",
    "\n",
    "            end_x = min(start_x + w, WIDTH-1)\n",
    "            capped_w = end_x - start_x\n",
    "            merged[-h:,start_x:end_x] = slot_image[:HEIGHT,:capped_w]\n",
    "            start_x += w\n",
    "        #plt.imshow(merged[:,:,::-1])\n",
    "        #plt.show()\n",
    "        dst_file = os.path.join(multi_dst_dir, f'{p}_{idx}.png')\n",
    "        cv2.imwrite(dst_file, merged)\n",
    "\n",
    "        new_row = {'image':dst_file, 'text':[caption], 'num_people':p, 'female':female_count, 'male':male_count}\n",
    "        multi_df = multi_df.append(new_row, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "407e9383",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_df.to_pickle(\"../data/deepfashion_23.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "e419d79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df, multi_df], ignore_index=True).to_pickle(\"../data/deepfashion_123.pickle\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vqgan_clip",
   "language": "python",
   "name": "vqgan_clip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
