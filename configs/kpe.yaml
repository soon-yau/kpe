transformer:
  target: dalle_pytorch.transformer.Transformer
  params:
    depth: 12
    dim: 512
    heads: 8
    dim_head: 64
    seq_len: 537
    rotary_emb: False
    image_fmap_size: 32
 
#transformer:
#  target: core.transformer.PyTorchTransformer
#  params:
#    num_layers: 12
#    d_model: 512
#    num_heads: 8
#    ffwd_dim: 2048
 
text_encoder:
  target: core.tokenizer.HugTokenizer
  params:
    bpe_path: tokenizer-deepfashion.json
    text_len: 256
    truncate_text: True

pose_encoder:
  target: core.pose_utils.KPE
  params:
    max_num_people: 3

image_encoder:
  target: core.vae.VQGanVAE
  params:
    vqgan_model_path: checkpoints/vae/vqgan_deepfashion.ckpt
    vqgan_config_path: checkpoints/vae/vqgan_deepfashion.yaml

train_dataset:
  target: core.loader.PoseDatasetPickle
  params:
    pickle_file: data/deepfashion_123_train.pickle
    folder: data
    pose_format: keypoint

train_loader:
  batch_size: 16
  num_workers: 8
  drop_last: True

val_dataset:
  target: core.loader.PoseDatasetPickle
  params:
    pickle_file: data/deepfashion_123_test.pickle
    folder: data
    pose_format: keypoint

val_loader:
  batch_size: 16
  num_workers: 8
  drop_last: True
  
loss_constant:
  text: 1
  pose: 10
  image: 7

optimizer:
  target: torch.optim.Adam
  params:
    lr: 6e-4

scheduler:
  target: torch.optim.lr_scheduler.ReduceLROnPlateau
  params:
    mode: min
    factor: 0.5
    patience: 3
    cooldown: 0
    min_lr: 0.000001
    threshold: 0.1
    verbose: True

trainer:
  max_epochs: 50
  #num_sanity_val_steps: 0
  gradient_clip_val: 0.5
  #val_check_interval: 200
  #distributed_backend: ddp
  #gpus: [0]

logging:
  image_frequency: 200
  max_images: 4